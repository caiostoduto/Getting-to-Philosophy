{
	"cells": [
		{
			"cell_type": "code",
			"execution_count": 1,
			"id": "969d34d4-9d21-4d51-b205-ada6568ef70c",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Standard and third-party library imports\n",
				"import pandas as pd\n",
				"from os.path import join\n",
				"from tqdm import tqdm  # For displaying progress bars\n",
				"from sys import stdout\n",
				"\n",
				"from typing import List  # For type hinting in functions that return lists\n",
				"\n",
				"# Define the namespaces to be excluded\n",
				"EXCLUDED_PREFIXES = [\n",
				"    \"Wikipédia:\",  # Wikipedia internal pages\n",
				"    \"Categoria:\",  # Category pages\n",
				"    \"Predefinição:\",  # Template pages\n",
				"    \"Ficheiro:\",  # File pages\n",
				"    \"Portal:\",  # Portal pages\n",
				"    \"Módulo:\",  # Module pages\n",
				"    \"Tópico:\",  # Topic pages\n",
				"    \"Ajuda:\",  # Help pages\n",
				"    \"MediaWiki:\",  # MediaWiki system pages\n",
				"    \"Livro:\",  # Book pages\n",
				"    \"TimedText:\",  # Timed text pages\n",
				"]\n",
				"\n",
				"\n",
				"def filter_redirects(redirect_array: List[str], page_title_lower: set):\n",
				"    \"\"\"\n",
				"    Filter out redirects that are not valid or relevant based on the page titles and excluded prefixes.\n",
				"\n",
				"    Args:\n",
				"        redirect_array (List[str]): A list of redirect links from a Wikipedia page.\n",
				"        page_title_lower (set): A set of lowercased page titles to filter against.\n",
				"\n",
				"    Returns:\n",
				"        List[str]: A list of valid redirect references after filtering.\n",
				"    \"\"\"\n",
				"\n",
				"    result = []\n",
				"    for redirect in redirect_array:\n",
				"        # Extract the main part of the redirect reference (before | or #)\n",
				"        redirect_ref = redirect.split(\"|\")[0].split(\"#\")[0].strip()\n",
				"\n",
				"        # Check if the redirect does not start with any excluded prefix\n",
				"        if not any(redirect_ref.startswith(prefix) for prefix in EXCLUDED_PREFIXES):\n",
				"            # If the reference exists in the page titles (case-insensitive), add to results\n",
				"            if redirect_ref.lower() in page_title_lower:\n",
				"                result.append(redirect_ref)\n",
				"\n",
				"    return result\n",
				"\n",
				"\n",
				"def process_page(wikinamedate: str) -> None:\n",
				"    \"\"\"\n",
				"    Process a raw Parquet file to filter page data and save the results to another Parquet file.\n",
				"\n",
				"    Args:\n",
				"        wikinamedate (str): A string used to locate the input Parquet file and name the output file.\n",
				"    \"\"\"\n",
				"    # Read the raw Parquet file for the given wikinamedate\n",
				"    print(f\"[INFO] Reading '{wikinamedate.replace('/', '-')}/raw.parquet'\")\n",
				"    df = pd.read_parquet(\n",
				"        join(\"../output/\", wikinamedate.replace(\"/\", \"-\"), \"raw.parquet\")\n",
				"    )\n",
				"\n",
				"    # Filter rows to include only those in the main namespace (Namespace 0)\n",
				"    print(\"[INFO] Filtering namespace\")\n",
				"    df = df[df[\"Page Namespace\"] == \"0\"]\n",
				"\n",
				"    # Prepare for progress tracking in filtering redirects\n",
				"    tqdm.pandas(\n",
				"        desc=\"[INFO] Filtering redirects\", unit_scale=True, unit=\" pages\", file=stdout\n",
				"    )\n",
				"\n",
				"    # Create a set of lowercased page titles to use for filtering redirects\n",
				"    page_title_lower = set(df[\"Page Title\"].str.lower().values)\n",
				"\n",
				"    # Apply the filtering function to each row's \"Page References\" field\n",
				"    df[\"Page References\"] = df[\"Page References\"].progress_apply(\n",
				"        lambda redirect_array: filter_redirects(redirect_array, page_title_lower)\n",
				"    )\n",
				"\n",
				"    # Save the processed data to a new Parquet file\n",
				"    df.to_parquet(\n",
				"        join(\"../output/\", wikinamedate.replace(\"/\", \"-\"), \"processed.parquet\")\n",
				"    )\n",
				"\n",
				"    # Log the number and percentage of pages that have at least one valid Page Reference\n",
				"    print(\n",
				"        f\"[INFO] {df['Page References'].apply(lambda x: len(x) > 0).sum()} out of {df['Page Title'].count()} ({round(100 * (df['Page References'].apply(lambda x: len(x) > 0).sum()/df['Page Title'].count()), 2)}%) pages have at least one Page Reference\"\n",
				"    )\n",
				"\n",
				"    # Clean up to free memory\n",
				"    del page_title_lower, df\n",
				"\n",
				"    print()\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 2,
			"id": "cca756e9-7e6f-4575-b42c-773b5ca35412",
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"[INFO] Reading 'ptwiki-20240720/raw.parquet'\n",
						"[INFO] Filtering namespace\n",
						"[INFO] Filtering redirects: 100%|██████████| 1.91M/1.91M [04:52<00:00, 6.53k pages/s]\n",
						"[INFO] 1135416 out of 1909778 (59.45%) pages have at least one Page Reference\n",
						"\n",
						"CPU times: user 5min 53s, sys: 8.89 s, total: 6min 2s\n",
						"Wall time: 5min 46s\n"
					]
				}
			],
			"source": [
				"%%time\n",
				"# Restore variable from different Jupyter notebook\n",
				"%store -r wikinamedate\n",
				"\n",
				"# Entry point: process a raw Parquet file to filter page data and save the results to another Parquet file.\n",
				"process_page(wikinamedate)\n"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Python 3 (ipykernel)",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.11.6"
		},
		"widgets": {
			"application/vnd.jupyter.widget-state+json": {
				"state": {},
				"version_major": 2,
				"version_minor": 0
			}
		}
	},
	"nbformat": 4,
	"nbformat_minor": 5
}
