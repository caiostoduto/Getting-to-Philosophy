{
	"cells": [
		{
			"cell_type": "markdown",
			"id": "53c88add-fda4-436b-893d-5c0c18f02ec6",
			"metadata": {},
			"source": [
				"# Wikipedia Dump Parser and Indexer\n",
				"\n",
				"This Jupyter notebook contains Python code for parsing and indexing Wikipedia dump files. The script performs the following main tasks:\n",
				"\n",
				"1. Counts the number of pages in the XML dump file\n",
				"2. Extracts relevant information from each page\n",
				"3. Processes revision links within page content\n",
				"4. Saves the extracted data into a Parquet file for efficient storage and querying\n",
				"\n",
				"## Features:\n",
				"\n",
				"- Efficient XML parsing using ElementTree\n",
				"- Regex-based extraction of revision links\n",
				"- Memory-efficient processing using chunking\n",
				"- Progress bar for tracking page processing\n",
				"- Output in Parquet format for optimized storage and query performance\n",
				"\n",
				"## Dependencies:\n",
				"\n",
				"- os\n",
				"- sys\n",
				"- subprocess\n",
				"- tqdm\n",
				"- xml.etree.ElementTree\n",
				"- pandas\n",
				"- pyarrow\n",
				"- pyre2 (falls back to re if not available)\n",
				"\n",
				"This script is designed to work with the Wikipedia dump files downloaded and extracted in the previous section. It processes the XML data and creates a structured dataset for further analysis."
			]
		},
		{
			"cell_type": "code",
			"execution_count": 1,
			"id": "548a8f2a-1940-4b40-9eb3-c3bc45a317f4",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Standard and third-party library imports\n",
				"from os import path, makedirs\n",
				"\n",
				"from sys import stdout\n",
				"from subprocess import check_output\n",
				"from tqdm import tqdm  # For displaying progress bars\n",
				"import xml.etree.ElementTree as ET\n",
				"import pandas as pd\n",
				"import pyarrow as pa\n",
				"import pyarrow.parquet as pq\n",
				"\n",
				"from typing import List  # For type hinting in functions that return lists\n",
				"\n",
				"# Try importing the 're2' module for better regex performance; fall back to 're' if unavailable\n",
				"# https://pypi.org/project/pyre2/\n",
				"try:\n",
				"    import re2 as re\n",
				"except ImportError:\n",
				"    import re\n",
				"\n",
				"    print(\"[WARN] Not using re2\")\n",
				"\n",
				"\n",
				"def count_pages(filename: str) -> int:\n",
				"    \"\"\"\n",
				"    Count the number of <page> elements in the XML file.\n",
				"\n",
				"    Args:\n",
				"        filename (str): The name of the XML file to count pages in.\n",
				"\n",
				"    Returns:\n",
				"        int: The number of <page> elements in the file.\n",
				"    \"\"\"\n",
				"\n",
				"    print(f\"[INFO] Counting how many \\\"<pages>\\\" in '{filename}'\")\n",
				"\n",
				"    # Run a shell command to count the occurrences of \"<page>\" in the file\n",
				"    # -w: Match whole words only\n",
				"    # -c: Print the number of matching lines\n",
				"    # https://unix.stackexchange.com/questions/398413/counting-occurrences-of-word-in-text-file\n",
				"    # https://linux.die.net/man/1/wc\n",
				"    # https://docs.python.org/3/library/subprocess.html\n",
				"    command = [\"grep\", \"-wc\", \"<page>\", path.join(\"../multistream/decompressed/\", filename)]\n",
				"    output = check_output(command).decode(stdout.encoding).strip()\n",
				"\n",
				"    return int(output)\n",
				"\n",
				"\n",
				"def revision(text: str) -> List[str]:\n",
				"    \"\"\"\n",
				"    Extract revision links from the given text.\n",
				"\n",
				"    Args:\n",
				"        text (str): The text from which to extract revision links.\n",
				"\n",
				"    Returns:\n",
				"        List[str]: A list of revision links found in the text.\n",
				"    \"\"\"\n",
				"\n",
				"    if text:\n",
				"        # Find and remove nested \"[[...]]\" links\n",
				"        while True:\n",
				"            i = start = text.find(\"[[\")\n",
				"            while i != -1:\n",
				"                # Find the next closing brackets or exit the loop\n",
				"                if text[i + 2 :].find(\"[[\") < text[i + 2 :].find(\"]]\"):\n",
				"                    i = text[i + 2 :].find(\"]]\") + i + 2\n",
				"                else:\n",
				"                    break\n",
				"\n",
				"            # Exit if no more nested brackets are found\n",
				"            if start == i:\n",
				"                break\n",
				"            else:\n",
				"                text = text[:start] + text[text[i + 2 :].find(\"]]\") + i + 4 :]\n",
				"\n",
				"        # Use regex to find all revision links in the cleaned text\n",
				"        return re.findall(\n",
				"            r\"(?<!>)\\[\\[(.*?)\\]\\]\", re.sub(r\"{{.*?}}\", \"\", re.sub(r\"\\(.*?\\)\", \"\", text))\n",
				"        )\n",
				"    else:\n",
				"        return []\n",
				"\n",
				"\n",
				"def index_pages(filename: str, wikinamedate: str) -> None:\n",
				"    \"\"\"\n",
				"    Process the XML file to extract page information and save it to a Parquet file.\n",
				"\n",
				"    Args:\n",
				"        filename (str): The name of the XML file to process.\n",
				"        wikinamedate (str): A string used to name the output Parquet file.\n",
				"    \"\"\"\n",
				"\n",
				"    total_pages = count_pages(filename)  # Count the total number of pages in the file\n",
				"    # Create an iterator for parsing XML elements\n",
				"    context = iter(\n",
				"        ET.iterparse(path.join(\"../multistream/decompressed/\", filename), events=(\"end\",))\n",
				"    )\n",
				"    chunk_size = 100_000  # Number of rows per chunk to be processed\n",
				"    rows = []  # List to hold data for current chunk\n",
				"    pqwriter = None  # Initialize Parquet writer\n",
				"\n",
				"    # Create necessary directory for storing output files.\n",
				"    # https://docs.python.org/3/library/os.html#os.makedirs\n",
				"    makedirs(path.join(\"../output/\", wikinamedate.replace(\"/\", \"-\")), exist_ok=True)\n",
				"\n",
				"    with tqdm(\n",
				"        total=total_pages,\n",
				"        unit=\" pages\",\n",
				"        unit_scale=True,\n",
				"        desc=\"[INFO] Processing pages\",\n",
				"        initial=0,\n",
				"        file=stdout,\n",
				"    ) as pbar:\n",
				"        title, id, namespace, redirect = [\n",
				"            None\n",
				"        ] * 4  # Initialize variables for storing page details\n",
				"\n",
				"        # Process XML elements\n",
				"        for event, elem in context:\n",
				"            match elem.tag:\n",
				"                case \"{http://www.mediawiki.org/xml/export-0.11/}title\":\n",
				"                    title = elem.text\n",
				"                case \"{http://www.mediawiki.org/xml/export-0.11/}ns\":\n",
				"                    namespace = elem.text\n",
				"                case \"{http://www.mediawiki.org/xml/export-0.11/}id\":\n",
				"                    if id is None:\n",
				"                        id = elem.text\n",
				"                case \"{http://www.mediawiki.org/xml/export-0.11/}redirect\":\n",
				"                    redirect = elem.attrib[\"title\"]\n",
				"                case \"{http://www.mediawiki.org/xml/export-0.11/}text\":\n",
				"                    # Append page details and revision links to the rows list\n",
				"                    if redirect is None:\n",
				"                        rows.append([title, id, namespace, revision(elem.text), False])\n",
				"                    else:\n",
				"                        rows.append([title, id, namespace, [redirect], True])\n",
				"\n",
				"                    id = None  # Reset ID for the next page\n",
				"                    redirect = None  # Reset redirect for the next page\n",
				"\n",
				"                    pbar.update()  # Update progress bar\n",
				"\n",
				"            elem.clear()  # Clear the element to free memory\n",
				"\n",
				"            # Write chunk to Parquet file if chunk size is reached\n",
				"            if len(rows) >= chunk_size:\n",
				"                df_chunk = pd.DataFrame(\n",
				"                    rows,\n",
				"                    columns=[\n",
				"                        \"Page Title\",\n",
				"                        \"Page ID\",\n",
				"                        \"Page Namespace\",\n",
				"                        \"Page References\",\n",
				"                        \"Page Redirect\",\n",
				"                    ],\n",
				"                )\n",
				"                table = pa.Table.from_pandas(df_chunk)\n",
				"\n",
				"                # Create a Parquet writer if not already created\n",
				"                if pqwriter is None:\n",
				"                    pqwriter = pq.ParquetWriter(\n",
				"                        path.join(\n",
				"                            \"../output/\", wikinamedate.replace(\"/\", \"-\"), \"raw.parquet\"\n",
				"                        ),\n",
				"                        table.schema,\n",
				"                    )\n",
				"\n",
				"                pqwriter.write_table(table)  # Write the chunk to the Parquet file\n",
				"\n",
				"                rows = []  # Clear rows to free memory\n",
				"\n",
				"    # Save remaining rows if any\n",
				"    if rows:\n",
				"        # Write chunk to Parquet file if chunk size is reached\n",
				"        df_chunk = pd.DataFrame(\n",
				"            rows,\n",
				"            columns=[\n",
				"                \"Page Title\",\n",
				"                \"Page ID\",\n",
				"                \"Page Namespace\",\n",
				"                \"Page References\",\n",
				"                \"Page Redirect\",\n",
				"            ],\n",
				"        )\n",
				"        table = pa.Table.from_pandas(df_chunk)\n",
				"\n",
				"        # Create a Parquet writer if not already created\n",
				"        if pqwriter is None:\n",
				"            pqwriter = pq.ParquetWriter(\n",
				"                path.join(\"../output/\", wikinamedate.replace(\"/\", \"-\"), \"raw.parquet\"),\n",
				"                table.schema,\n",
				"            )\n",
				"\n",
				"        pqwriter.write_table(table)  # Write the chunk to the Parquet file\n",
				"\n",
				"    # Close the Parquet writer if it was opened\n",
				"    if pqwriter:\n",
				"        pqwriter.close()\n",
				"\n",
				"    # Clean up variables to free memory\n",
				"    del (\n",
				"        title,\n",
				"        id,\n",
				"        namespace,\n",
				"        event,\n",
				"        elem,\n",
				"        context,\n",
				"        rows,\n",
				"        pqwriter,\n",
				"        chunk_size,\n",
				"        df_chunk,\n",
				"        table,\n",
				"    )\n",
				"\n",
				"    print()\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 2,
			"id": "0f9feb00-9b49-417f-9886-bf260eea87cc",
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"[INFO] Counting how many \"<pages>\" in 'ptwiki-20240720-pages-articles-multistream.xml'\n",
						"[INFO] Processing pages: 100%|██████████| 2.63M/2.63M [19:15<00:00, 2.28k pages/s]\n",
						"\n",
						"CPU times: user 19min, sys: 23.1 s, total: 19min 23s\n",
						"Wall time: 19min 41s\n"
					]
				}
			],
			"source": [
				"%%time\n",
				"# Restore variable from different Jupyter notebook\n",
				"%store -r filename wikinamedate\n",
				"\n",
				"# Entry point: process the XML file to extract page information and save it to a Parquet file.\n",
				"index_pages(filename, wikinamedate)\n"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Python 3 (ipykernel)",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.11.6"
		},
		"widgets": {
			"application/vnd.jupyter.widget-state+json": {
				"state": {},
				"version_major": 2,
				"version_minor": 0
			}
		}
	},
	"nbformat": 4,
	"nbformat_minor": 5
}
