{
	"cells": [
		{
			"cell_type": "markdown",
			"id": "3968392b",
			"metadata": {},
			"source": [
				"# Wikipedia Page Reference Analysis\n",
				"\n",
				"This Jupyter notebook contains Python code for analyzing the reference structure of Wikipedia pages. It includes two main components:\n",
				"\n",
				"1. A `Counter` class for processing and analyzing page references\n",
				"2. Interactive cells for user input and result display\n",
				"\n",
				"## Features:\n",
				"\n",
				"- Efficient data handling using Pandas and Parquet\n",
				"- Analysis of reference degrees (how many steps away pages are from a given page)\n",
				"- Calculation of weighted mean distance, total nodes, and network diameter\n",
				"- Option to consider only the first reference or all references for each page\n",
				"\n",
				"## Dependencies:\n",
				"\n",
				"- pandas\n",
				"- os\n",
				"- typing\n",
				"\n",
				"## Note:\n",
				"\n",
				"The analysis can be performed considering either only the first reference of each page or all references. This allows for different perspectives on the Wikipedia link structure."
			]
		},
		{
			"cell_type": "code",
			"execution_count": 1,
			"id": "f579ebee-eda4-45b5-ae05-e7c4fc4c22c1",
			"metadata": {},
			"outputs": [],
			"source": [
				"import pandas as pd\n",
				"from os.path import join\n",
				"\n",
				"from typing import List, Set  # For type hinting\n",
				"\n",
				"\n",
				"def get_ordinal_suffix(n: int) -> str:\n",
				"    \"\"\"\n",
				"    Get the ordinal suffix for a given integer.\n",
				"\n",
				"    Args:\n",
				"        n (int): The integer for which to determine the ordinal suffix.\n",
				"\n",
				"    Returns:\n",
				"        str: The ordinal suffix (e.g., 'st', 'nd', 'rd', 'th').\n",
				"    \"\"\"\n",
				"\n",
				"    if 10 <= n % 100 <= 20:\n",
				"        return \"th\"\n",
				"    return {1: \"st\", 2: \"nd\", 3: \"rd\"}.get(n % 10, \"th\")\n",
				"\n",
				"\n",
				"def weighted_mean(values: List[int]) -> float:\n",
				"    \"\"\"\n",
				"    Calculate the weighted mean of a list of numbers.\n",
				"\n",
				"    Parameters:\n",
				"    values (list of float or int): List of numbers to calculate the weighted mean.\n",
				"\n",
				"    Returns:\n",
				"    float: The weighted mean of the list.\n",
				"    \"\"\"\n",
				"\n",
				"    # Calculate weighted sum\n",
				"    weighted_sum = sum((i + 1) * value for i, value in enumerate(values))\n",
				"\n",
				"    # Calculate the sum of weights (positions)\n",
				"    sum_of_weights = sum(value for value in values)\n",
				"\n",
				"    # Calculate the weighted mean\n",
				"    return round(weighted_sum / sum_of_weights, 2)\n",
				"\n",
				"\n",
				"class Counter:\n",
				"    df_filtered: pd.DataFrame\n",
				"\n",
				"    def __init__(self, df: pd.DataFrame):\n",
				"        \"\"\"\n",
				"        Initialize the Counter with a filtered DataFrame.\n",
				"\n",
				"        Args:\n",
				"            df (pd.DataFrame): The DataFrame containing Wikipedia page data.\n",
				"        \"\"\"\n",
				"        print(\"[INFO] Applying modifiers and filters to DataFrame\")\n",
				"\n",
				"        # Filter out rows where 'Page References' is NaN or empty\n",
				"        self.df_filtered = df[\n",
				"            df[\"Page References\"].notna() & df[\"Page References\"].str.len().gt(0)\n",
				"        ].copy()\n",
				"\n",
				"        # Convert 'Page Title' to lowercase and ensure 'Page References' are lists of lowercase references\n",
				"        self.df_filtered[\"Page Title\"] = self.df_filtered[\"Page Title\"].str.lower()\n",
				"        self.df_filtered[\"Page References\"] = self.df_filtered[\"Page References\"].apply(\n",
				"            lambda refs: [ref.lower() for ref in refs]\n",
				"        )\n",
				"\n",
				"    def count(self, page_title: str, first_ref: bool):\n",
				"        return count(self.df_filtered, page_title, first_ref)\n",
				"\n",
				"\n",
				"def read_parquet(wikinamedate: str) -> None:\n",
				"    \"\"\"\n",
				"    Reads a processed Parquet file containing Wikipedia page data and returns it as a DataFrame.\n",
				"\n",
				"    Args:\n",
				"        wikinamedate (str): The date and name string used to locate the Parquet file.\n",
				"\n",
				"    Returns:\n",
				"        pd.DataFrame: The DataFrame containing the processed page data.\n",
				"    \"\"\"\n",
				"\n",
				"    print(f\"[INFO] Reading '{wikinamedate.replace('/', '-')}/processed.parquet'\")\n",
				"    df = pd.read_parquet(\n",
				"        join(\"../output/\", wikinamedate.replace(\"/\", \"-\"), \"processed.parquet\")\n",
				"    )\n",
				"\n",
				"    return df\n",
				"\n",
				"\n",
				"def count(df: pd.DataFrame, page_title: str, first_ref: bool) -> None:\n",
				"        \"\"\"\n",
				"        Counts the occurrences of a specific page title in the DataFrame's 'Page References' column,\n",
				"        iteratively updating the count for multiple degrees of reference.\n",
				"\n",
				"        Args:\n",
				"            df (pd.DataFrame): The DataFrame containing Wikipedia page data.\n",
				"            page_title (str): The title of the page to start counting references from.\n",
				"            first_ref (bool): Whether to only consider the first reference in each list of references.\n",
				"\n",
				"        Returns:\n",
				"            None: This function prints the count and does not return any value.\n",
				"        \"\"\"\n",
				"\n",
				"        df_filtered = df.copy()\n",
				"        # If first_ref is True, only consider the first reference in each list\n",
				"        if first_ref:\n",
				"            df_filtered[\"Page References\"] = df_filtered[\"Page References\"].apply(\n",
				"                lambda refs: [refs[0]]\n",
				"            )\n",
				"\n",
				"        # Initialize the sets for processed pages and pages to process\n",
				"        processed_pages: Set[str] = set()\n",
				"        pages_to_process: Set[str] = {page_title.lower()}\n",
				"        counts: List[int] = []\n",
				"\n",
				"        # Process pages in degrees of reference\n",
				"        while pages_to_process:\n",
				"            degree = len(counts) + 1\n",
				"\n",
				"            print(\n",
				"                f\"Checking {degree}{get_ordinal_suffix(degree)} degree of distance\",\n",
				"                end=\"\\r\",\n",
				"            )\n",
				"            # Find all rows where any reference is in pages_to_process\n",
				"            current_batch = df_filtered[\n",
				"                df_filtered[\"Page References\"].apply(\n",
				"                    lambda refs: any(ref in pages_to_process for ref in refs)\n",
				"                )\n",
				"            ]\n",
				"\n",
				"            # Update the set of pages to process in the next iteration\n",
				"            new_pages = set(current_batch[\"Page Title\"]) - processed_pages\n",
				"\n",
				"            # Add current batch count to the counter\n",
				"            counts.append(len(new_pages))\n",
				"\n",
				"            # Update processed pages\n",
				"            processed_pages.update(new_pages)\n",
				"\n",
				"            # Update pages_to_process with new pages\n",
				"            pages_to_process = new_pages\n",
				"\n",
				"        print(f\"Degree Series: {str(counts)}\")\n",
				"        print(f\"Mean Distance: {weighted_mean(counts)} nodes\")\n",
				"        print(\n",
				"            f\"Total Nodes: {str(sum(counts))} ({round(100 * sum(counts)/df_filtered['Page Title'].count(), 2)}%)\"\n",
				"        )\n",
				"        print(f\"Diameter: {str(len(counts))} nodes\")\n",
				"        print(f\"First Reference: {str(first_ref)}\")\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 2,
			"id": "f180bc0d-677d-4dcf-93c6-30adaaf7e92c",
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"[INFO] Reading 'ptwiki-20240720/processed.parquet'\n",
						"[INFO] Applying modifiers and filters to DataFrame\n",
						"\n",
						"CPU times: user 53.3 s, sys: 4.74 s, total: 58.1 s\n",
						"Wall time: 53.7 s\n"
					]
				}
			],
			"source": [
				"%%time\n",
				"# Restore variable from different Jupyter notebook\n",
				"%store -r wikinamedate\n",
				"\n",
				"# Reads a processed Parquet file containing Wikipedia page data\n",
				"df = read_parquet(wikinamedate)\n",
				"counter = Counter(df)\n",
				"# del df # (maybe you should uncomment this)\n",
				"print()\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 3,
			"id": "acbca10b-6a82-4be0-8ec9-ce4adb6f70cf",
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Type a Page Title: Filosofia\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n",
						"Degree Series: [386, 1102, 1090, 3039, 6954, 9894, 46271, 19748, 20979, 15232, 27173, 27683, 24657, 17524, 12703, 9222, 6269, 4558, 4019, 1777, 965, 418, 207, 82, 16, 1, 0]\n",
						"Mean Distance: 10.72 nodes\n",
						"Total Nodes: 261969 (13.75%)\n",
						"Diameter: 27 nodes\n",
						"\n",
						"CPU times: user 1min 18s, sys: 366 ms, total: 1min 19s\n",
						"Wall time: 1min 21s\n"
					]
				}
			],
			"source": [
				"%%time\n",
				"\n",
				"page_title = input('Type a Page Title:')\n",
				"print()\n",
				"\n",
				"counter.count(page_title, first_ref=True)\n",
				"print()\n"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Python 3 (ipykernel)",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.11.6"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 5
}
