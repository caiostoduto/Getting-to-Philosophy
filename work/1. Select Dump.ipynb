{
	"cells": [
		{
			"cell_type": "markdown",
			"id": "7c4640c2-69ca-4c89-aadf-e4853d8e5f86",
			"metadata": {},
			"source": [
				"## Extração dos dumps"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 1,
			"id": "dcd6fbe0-399f-47bd-92d1-716485c29a7a",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Standard and third-party library imports\n",
				"import hashlib\n",
				"import os\n",
				"import requests\n",
				"import sys\n",
				"import shutil\n",
				"import subprocess\n",
				"from tqdm import tqdm  # For displaying progress bars\n",
				"\n",
				"from typing import (\n",
				"    Tuple,\n",
				"    Dict,\n",
				")  # For type hinting in functions that return multiple values or dictionaries\n",
				"\n",
				"\n",
				"def create_multistream_dir() -> None:\n",
				"    \"\"\"\n",
				"    Create necessary directories for storing compressed and decompressed multistream files.\n",
				"    \"\"\"\n",
				"\n",
				"    dirs = [\"../multistream/compressed\", \"../multistream/decompressed\"]\n",
				"\n",
				"    for path in dirs:\n",
				"        # Create dir recursively\n",
				"        # https://docs.python.org/3/library/os.html#os.makedirs\n",
				"        os.makedirs(path, exist_ok=True)\n",
				"\n",
				"\n",
				"def get_articles_details(dump_url: str) -> Dict[str, dict]:\n",
				"    \"\"\"\n",
				"    Fetch the dump status and details from the provided URL and check if the dump is completed.\n",
				"\n",
				"    Args:\n",
				"        dump_url (str): URL to fetch dump details.\n",
				"\n",
				"    Returns:\n",
				"        Dict[str, dict]: Details of the articles multistream dump.\n",
				"    \"\"\"\n",
				"\n",
				"    print(\"[INFO] Fetching dump info\")\n",
				"\n",
				"    # https://requests.readthedocs.io/en/latest/user/quickstart/#make-a-request\n",
				"    resp = requests.get(url=dump_url)\n",
				"    # https://requests.readthedocs.io/en/latest/user/quickstart/#json-response-content\n",
				"    data = resp.json()\n",
				"\n",
				"    # (ex) See https://dumps.wikimedia.org/ptwiki/20240720/dumpstatus.json\n",
				"    articles_details = data[\"jobs\"][\"articlesmultistreamdumprecombine\"]\n",
				"\n",
				"    # Raise an exception if the dump is not completed ('done')\n",
				"    check_articles_status(articles_details)\n",
				"\n",
				"    return articles_details\n",
				"\n",
				"\n",
				"def check_articles_status(articles_details: Dict[str, str]) -> None:\n",
				"    \"\"\"\n",
				"    Raise an exception if the dump status is not 'done'.\n",
				"\n",
				"    Args:\n",
				"        articles_details (Dict[str, str]): Details of the articles multistream dump.\n",
				"\n",
				"    Raises:\n",
				"        Exception: If the status of the dump is not 'done'.\n",
				"    \"\"\"\n",
				"\n",
				"    if articles_details[\"status\"] != \"done\":\n",
				"        raise Exception(\n",
				"            f\"'Article Multistream Dump Precombine' is not 'done' ({articles_details['status']})\"\n",
				"        )\n",
				"\n",
				"\n",
				"def get_file(dump_url: str) -> Tuple[Dict[str, str], str]:\n",
				"    \"\"\"\n",
				"    Retrieve the file details of the multistream dump and its filename.\n",
				"\n",
				"    Args:\n",
				"        dump_url (str): URL to fetch dump details.\n",
				"\n",
				"    Returns:\n",
				"        Tuple[Dict[str, str], str]: A tuple containing the file details and the filename.\n",
				"    \"\"\"\n",
				"\n",
				"    # Fetch dump status and details for 'articlesmultistreamdumprecombine' and check if the dump is completed\n",
				"    articles_details = get_articles_details(dump_url)\n",
				"\n",
				"    # Find file from dumps.wikimedia.org from 'articlesmultistreamdumprecombine' that endings with '-pages-articles-multistream.xml.bz2'\n",
				"    # Because there is also a '-pages-articles-multistream-index.txt.bz2' file included\n",
				"    # (ex) See https://dumps.wikimedia.org/ptwiki/20240720/dumpstatus.json\n",
				"    key = [\n",
				"        file\n",
				"        for file in articles_details[\"files\"].keys()\n",
				"        if file.endswith(\"-pages-articles-multistream.xml.bz2\")\n",
				"    ][0]\n",
				"    file = articles_details[\"files\"][key]\n",
				"\n",
				"    # Extract the filename from the URL for later use\n",
				"    filename = get_filename(file)\n",
				"\n",
				"    return file, filename\n",
				"\n",
				"\n",
				"def get_filename(file: Dict[str, str]) -> str:\n",
				"    \"\"\"\n",
				"    Extract the filename from a file dictionary.\n",
				"\n",
				"    Args:\n",
				"        file (Dict[str, str]): Dictionary containing file details.\n",
				"\n",
				"    Returns:\n",
				"        str: Extracted filename from the URL.\n",
				"    \"\"\"\n",
				"\n",
				"    # (ex) /ptwiki/20240720/ptwiki-20240720-pages-articles-multistream.xml.bz2 ->\n",
				"    #   -> ptwiki-20240720-pages-articles-multistream.xml.bz2\n",
				"    return file[\"url\"].split(\"/\")[-1]\n",
				"\n",
				"\n",
				"def check_sha1(path: str, filename: str) -> str:\n",
				"    \"\"\"\n",
				"    Compute the SHA1 checksum of the file.\n",
				"\n",
				"    Args:\n",
				"        path (str): Path to the file.\n",
				"        filename (str): Name of the file (for logging purposes).\n",
				"\n",
				"    Returns:\n",
				"        str: SHA1 checksum of the file in hexadecimal format.\n",
				"    \"\"\"\n",
				"\n",
				"    print(f\"[INFO] Checking SHA1 Checksum of '{filename}'\")\n",
				"\n",
				"    # https://docs.python.org/3/library/hashlib.html#hashlib.sha1\n",
				"    sha1 = hashlib.sha1()\n",
				"\n",
				"    # https://stackoverflow.com/a/22058673\n",
				"    with open(path, \"rb\") as f:\n",
				"        while True:\n",
				"            data = f.read(65536)  # BUFF_SIZE (arbitrary value)\n",
				"            if not data:\n",
				"                break\n",
				"            sha1.update(data)\n",
				"\n",
				"    # Return SHA1 in hex format\n",
				"    # https://docs.python.org/3/library/hashlib.html#hashlib.hash.hexdigest\n",
				"    return sha1.hexdigest()\n",
				"\n",
				"\n",
				"def check_disk_space(file: Dict[str, str]) -> None:\n",
				"    \"\"\"\n",
				"    Check if there is enough disk space to download the file.\n",
				"\n",
				"    Args:\n",
				"        file (Dict[str, int]): Dictionary containing file details with size.\n",
				"\n",
				"    Raises:\n",
				"        Exception: If there is not enough disk space available.\n",
				"    \"\"\"\n",
				"\n",
				"    _, _, free = shutil.disk_usage(\"/\")\n",
				"\n",
				"    if free < file[\"size\"]:\n",
				"        raise Exception(f\"Not enough disk space ({str(file['size'])} / {str(free)})\")\n",
				"\n",
				"\n",
				"def download_multistream(file: Dict[str, str], filename: str, path: str) -> None:\n",
				"    \"\"\"\n",
				"    Download the multistream file and verify its integrity.\n",
				"\n",
				"    Args:\n",
				"        file (Dict[str, str]): Dictionary containing file details.\n",
				"        filename (str): Name of the file to be downloaded.\n",
				"        path (str): Path to save the downloaded file.\n",
				"    \"\"\"\n",
				"\n",
				"    # Ensure there's enough free disk space to download the file\n",
				"    check_disk_space(file)\n",
				"\n",
				"    # (ex) 'https://dumps.wikimedia.org' + '/ptwiki/20240720/ptwiki-20240720-pages-articles-multistream.xml.bz2'\n",
				"    url = \"https://dumps.wikimedia.org\" + file[\"url\"]\n",
				"\n",
				"    # Download file from dumps.wikimedia.org\n",
				"    with requests.get(url, stream=True) as stream:\n",
				"        # Check if the request was successful\n",
				"        # https://3.python-requests.org/api/#requests.Response.raise_for_status\n",
				"        stream.raise_for_status()\n",
				"        # https://stackoverflow.com/a/44299915\n",
				"        total_size = int(stream.headers.get(\"content-length\", 0))\n",
				"\n",
				"        # Initialize a progress bar to track the processing of pages\n",
				"        # https://tqdm.github.io/docs/tqdm/\n",
				"        with tqdm(\n",
				"            total=total_size,\n",
				"            unit=\"B\",\n",
				"            unit_scale=True,\n",
				"            desc=f\"[INFO] Downloading '{filename}'\",\n",
				"            initial=0,\n",
				"            file=sys.stdout,\n",
				"        ) as pbar:\n",
				"            with open(path, mode=\"wb\") as multistream_file:\n",
				"                for chunk in stream.iter_content(chunk_size=10 * 1024):\n",
				"                    multistream_file.write(chunk)\n",
				"                    pbar.update(len(chunk)) # Update progress bar\n",
				"\n",
				"    # Verify the integrity of the downloaded file by comparing its SHA1 checksum with the expected value\n",
				"    sha1_hex = check_sha1(path, filename)\n",
				"    if sha1_hex != file[\"sha1\"]:\n",
				"        raise Exception(f\"SHA1 Checksum did not match ({sha1_hex} != {file['sha1']})\")\n",
				"\n",
				"\n",
				"def get_multistream_file(wikinamedate: str) -> str:\n",
				"    \"\"\"\n",
				"    Ensure the multistream file is downloaded, decompressed, and ready for use.\n",
				"\n",
				"    Args:\n",
				"        wikinamedate (str): Wiki name and date to identify the dump.\n",
				"\n",
				"    Returns:\n",
				"        str: The filename of the decompressed multistream file.\n",
				"    \"\"\"\n",
				"\n",
				"    # (ex) 'https://dumps.wikimedia.org/' + 'ptwiki/20240720' + '/dumpstatus.json'\n",
				"    dump_url = \"https://dumps.wikimedia.org/\" + wikinamedate + \"/dumpstatus.json\"\n",
				"\n",
				"    # Retrieve details of the relevant multistream dump file (filtered for '-pages-articles-multistream.xml.bz2')\n",
				"    file, filename = get_file(dump_url)\n",
				"\n",
				"    # Check if file was already downloaded and decompressed\n",
				"    if os.path.isfile(os.path.join(\"../multistream/decompressed\", filename[:-4])):\n",
				"        print(\"[INFO] Valid matching multistream found in multistream folder\")\n",
				"    else:\n",
				"        # Check if file was already downloaded (waiting to be decompressed)\n",
				"        if os.path.isfile(os.path.join(\"../multistream/compressed\", filename)):\n",
				"            # Check if downloaded file isn't corrupted\n",
				"            if (\n",
				"                check_sha1(\n",
				"                    os.path.join(\"../multistream/compressed\", filename), filename\n",
				"                )\n",
				"                != file[\"sha1\"]\n",
				"            ):\n",
				"                print(\"[INFO] Invalid matching multistream found in multistream folder\")\n",
				"\n",
				"                # Delete file from '../multistream/compressed' folder\n",
				"                os.remove(os.path.join(\"../multistream/compressed\", filename))\n",
				"\n",
				"                # Download file again\n",
				"                download_multistream(\n",
				"                    file, filename, os.path.join(\"../multistream/compressed\", filename)\n",
				"                )\n",
				"            else:\n",
				"                print(\"[INFO] Valid matching multistream found in multistream folder\")\n",
				"        else:\n",
				"            # Download file\n",
				"            download_multistream(\n",
				"                file, filename, os.path.join(\"../multistream/compressed\", filename)\n",
				"            )\n",
				"\n",
				"        # Decompress the .bz2 file and move the resulting .xml file to the decompressed directory\n",
				"        extract_dump(filename)\n",
				"\n",
				"        # Delete file from '../multistream/compressed' folder\n",
				"        os.remove(os.path.join(\"../multistream/compressed\", filename))\n",
				"\n",
				"    # Remove .bz2 sufix from archive filename\n",
				"    # (ex) ptwiki-20240720-pages-articles-multistream.xml.bz2 -> ptwiki-20240720-pages-articles-multistream.xml\n",
				"    return filename[:-4]\n",
				"\n",
				"\n",
				"def delete_corrupted_xmls() -> None:\n",
				"    \"\"\"\n",
				"    Remove any corrupted or incomplete XML files from the compressed directory.\n",
				"    \"\"\"\n",
				"\n",
				"    for file in os.listdir(\"../multistream/compressed\"):\n",
				"        if file.endswith(\".xml\"):\n",
				"            os.remove(os.path.join(\"../multistream/compressed/\", file))\n",
				"\n",
				"\n",
				"def extract_dump(filename: str) -> None:\n",
				"    \"\"\"\n",
				"    Decompress the .bz2 file and move the resulting .xml file to the decompressed directory.\n",
				"\n",
				"    Args:\n",
				"        filename (str): Name of the .bz2 file to decompress.\n",
				"    \"\"\"\n",
				"\n",
				"    print(f\"[INFO] Extracting '{filename}' (this might take several minutes)\")\n",
				"\n",
				"    # Run 'bzip2 -dk ../multistream/compressed/...' to decompress the .bz2 file\n",
				"    # https://superuser.com/a/480951\n",
				"    subprocess.run(\n",
				"        [\"bzip2\", \"-dk\", \"../multistream/compressed/\" + filename], check=True\n",
				"    )\n",
				"\n",
				"    # https://docs.python.org/3/library/os.html#os.replace\n",
				"    # https://stackoverflow.com/a/8858026\n",
				"    os.replace(\n",
				"        os.path.join(\"../multistream/compressed/\", filename[:-4]),\n",
				"        os.path.join(\"../multistream/decompressed/\", filename[:-4]),\n",
				"    )\n",
				"\n",
				"\n",
				"def select_dump() -> Tuple[str, str]:\n",
				"    \"\"\"\n",
				"    Initialize directories, clean up corrupted files, and handle the selection, download, and extraction of the dump.\n",
				"\n",
				"    Returns:\n",
				"        Tuple[str, str]: A tuple containing the wiki name/date and the filename of the decompressed multistream file.\n",
				"    \"\"\"\n",
				"\n",
				"    # Create necessary directories for compressed and decompressed multistream files\n",
				"    create_multistream_dir()\n",
				"\n",
				"    # Remove any corrupted or incomplete XML files to avoid processing errors\n",
				"    delete_corrupted_xmls()\n",
				"\n",
				"    # Request user's input for a wikiname/date reference\n",
				"    wikinamedate = input(\"Fill with wikiname/date [ex: 'ptwiki/20240720']\")\n",
				"    print()\n",
				"\n",
				"    # Main logic for verifying, downloading, and extracting the multistream dump file\n",
				"    filename = get_multistream_file(wikinamedate)\n",
				"\n",
				"    print(f\"[INFO] '{wikinamedate}' successfully selected\")\n",
				"    print()\n",
				"\n",
				"    return wikinamedate, filename"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 2,
			"id": "473a6e1c-46ac-4f1c-b80d-6d46107b8e89",
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Fill with wikiname/date [ex: 'ptwiki/20240720'] ptwiki/20240720\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n",
						"[INFO] Fetching dump info\n",
						"[INFO] Downloading 'ptwiki-20240720-pages-articles-multistream.xml.bz2': 100%|██████████| 2.43G/2.43G [08:29<00:00, 4.77MB/s]\n",
						"[INFO] Checking SHA1 Checksum of 'ptwiki-20240720-pages-articles-multistream.xml.bz2'\n",
						"[INFO] Extracting 'ptwiki-20240720-pages-articles-multistream.xml.bz2' (this might take several minutes)\n",
						"[INFO] 'ptwiki/20240720' successfully selected\n",
						"\n",
						"Stored 'filename' (str)\n",
						"Stored 'wikinamedate' (str)\n",
						"CPU times: user 1min 23s, sys: 46.9 s, total: 2min 10s\n",
						"Wall time: 23min 11s\n"
					]
				}
			],
			"source": [
				"%%time\n",
				"# Entry point: initialize directories, delete corrupted files, and handle download and extraction of the selected dump\n",
				"wikinamedate, filename = select_dump()\n",
				"\n",
				"# Save variable between different Jupyter notebooks\n",
				"%store filename wikinamedate"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Python 3 (ipykernel)",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.11.6"
		},
		"widgets": {
			"application/vnd.jupyter.widget-state+json": {
				"state": {},
				"version_major": 2,
				"version_minor": 0
			}
		}
	},
	"nbformat": 4,
	"nbformat_minor": 5
}
